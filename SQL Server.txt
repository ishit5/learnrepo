Primary Key - A primary key is used to uniquely identify each record in a table
				
Creating tables :				
Create table tblGender
(
ID int NOT NULL Primary Key,
Gender char(01) NOT NULL
)				
				
To change the database :				
Use [<Database_Name>]
Go				
				
Foreign Key - A foreign key is used to maintain data integrity. It is used to enforce relationship between two tables				
Here, the primary key of one table becomes the foreign key of the other table				
				
The values of the foreign key in a FK_Table looks up its values in the primary key of the PK_Table and thus values which are not in the primary key column of the PK_Tables will not be allowed to enter in the foreign key of the FK_Table
				
Adding foreign key constraint in an existing table :				
Alter table <FK_Table> add constraint <constraint_name> foreign key <FK_Column>
references <PK_Table(<PK_Column>)>				
				
Adding primary key constraint in an existing table :				
Alter table <Table_Name> add constraint <Constraint_Name> primary key <Column_Name>				
				
Inserting values in a table :				
Insert into <Table_Name> (<Column_Names>) values (<Column_Values>)				
				
If we want to insert some of the column values only in the table and not all the column values, then we need to specify the name of the columns whose values needs to be inserted
If we want to insert all the column values of a table then we don't have to mention the column names (It is not mandatory)		
				
Adding default constraint to an existing column of an existing table :				
Alter table <Table_Name> add constraint <Constraint_Name> default <Default_Value>
for <Existing_Column_Name>				
				
Adding default constraint to a new column of an existing table :				
Alter table <Table_Name> add column <Column_Name> <Data_Type> <NULL/NOTNULL>
constraint <Constraint_Name> default <Default_Value>				
				
Dropping a constraint :				
Alter table <Table_Name> drop constraint <Constraint_Name>				
				
NOTE : The default only gets executed when we donot pass any value to the specific field which has a default constraint		
				
Cascading referential integrity - It states that if one table is connected to another table with a foreign key constraint relationship, then it is not possible to delete or update the parent record meaning the record which has references from the FK_Table				
By default, the cascading referential integrity option is set to No Action which means by default we cannot delete or update the parent record without deleting the child record first
There are 4 options to define the cascading rule :				
i> No Action - This is default option where one cannot delete or update the parent record without first deleting all the child records associated with it.
ii> Cascade - In this option, on updation or deletion of the parent record, all the associated child records will also be updated or deleted depending on the action i.e. Update or Delete.
iii> Set NULL - In this option, on updation or deletion of the parent record, all the assocated child records will be set to the NULL.	
iv> Set Default - If there is a default constraint in the foreign key of the foreign key table, then only we can use this option. Here, on updation or deletion of the parent record, all the assocated child records will be set to the default value.
				
NOTE : All the cascading referential integrity options will be available in the foreign key constraint only. Whenever you need to change the cascading option, you need to go to the foreign key constraint and then to insert and update specification.
				
Alt + F1 - To see the table related information				
				
Check Constraint - A check constraint is a constraint which is used to limit the values allowed to enter in any particular field. The check constraints allows NULL to be inserted or updated				
				
Adding a check constraint :				
Alter table <Table_Name> add constraint <Constraint_Name> 
check (<Boolean Expression>)				
Alter table tblPerson add constraint CK_tblPerson_Age
check (Age > 0 and Age < 150)				
				
Identity Column - An identity column is just an auto-incremented column whose value is not needed to be supplied externally. It auto generates an unique number by auto-incrementing some value with the previous generated number				
You can set this to True or False from the properties of that particular column. If it is set to True, then the values will be auto-generated otherwise the values needs to be passed externally
It has two parameters :				
Identity Increment - This is the value with which the next value is generated by incrementing the value assigned to this field		
Identity Seed - This is the base value which is generated for the very first row of the table				
				
Now, whenever we want to reuse an existing value which has been deleted from the table, then we cannot do that since the primary key is an identity column
Still if somebody wants to do the same, then he needs to set the IDENTITY_INSERT ON and also he needs to pass the column list for the values passed as well (MANDATORY) :
SET IDENTITY_INSERT <Table_Name> ON				
With this command you can supply values to identity column explicitly as well				
				
NOTE : You can only supply values which has already been generated by the identity column previously and also if you don't want to pass the identity values, then you need to turn the IDENTITY_INSERT OFF
SET IDENTITY_INSERT <Table_Name> OFF				
				
You can only pass values to the identity column when the IDENTITY_INSERT is ON, otherwise you cannot pass values explicitly	
Once you turn that ON, then you cannot opt to not pass any value to the identity column untill and unless the IDENTITY_INSERT is turned OFF
				
Resetting the Identity Column value :				
Let's say we delete all the records of a table whose PK is an identity field, then when we insert a new record in the table, the value generated for that record will be the incremented value of the last generated value of the identity field
So, if we want to reset the identity value, we can do that by using the DBCC command (Data Base Consistency Check) :		
DBCC CHECKIDENT(<Table_Name>, RESEED, 0)				
Here, we are resetting the seed to 0 so that when any new record gets inserted in the table the identity column should be able to start assigning the values with the base number defined.
				
Getting last value generated from the identity column :				
In order to get the last generated value from the identity column, we have three ways to do it :				
i> SCOPE_IDENTITY() - This returns the identity value from the same session and the same scope				
ii> @@IDENTITY - This returns the identity value from the same session but across any scope				
iii> IDENT_CURRENT('<Table_Name>') - This returns the identity value from any session and across any scope		
				
Since, IDENT_CURRENT takes a parameter which is nothing but the table name, hense it will return the last generated identity value of that particular table
				
Unique Key - A Unique is a key which is used to maintain unique values in a table. Now, function of PK is also the same but the primary difference between the two is you cannot have more than one PK in a table but you can have multiple UK				
We use Unique Keys (Unique Key Constraint) when we have to store values of fields such as SSN or E-Mail (basically the fields which should not allow duplicate values in it)
Another difference between PK and UK is Primary Key doesnot allow NULLS but UK allows only one NULL			
				
Adding Unique Key constraint in an existing table :				
Alter Table <Table_Name> add constraint <Constraint_Name> Unique(<Column_Name>)				
				
Fully Qualified Table Name :				
<database_name>.<schema_name>.<table_name>				
Ex - Select * from Sample.dbo.Table1				
This is used when you want to get the details of a table which is not present in the current database selected. On these types of cases, when you give a fully qualified table name, then it searches the table in the mentioned database only
dbo is the Default schema
		
Distinct :				
Distinct gives non-repeating values for the fields given in the select statement of a select query				
Select distinct city means - All the unique cities of a dataset				
Select distinct city, name means - All the unique combination of city and name present in the dataset			
				
Priority of Execution :				
From - Looks for the table in the database				
Where - Filters records based on the criterion				
Group by - Groups the dataset				
Having - Filters records based on group conditions				
Order by - Orders the data in either ascending or descending fashion				
Select - Prints the data to the output section				
				
Pattern Matching :				
For pattern matching, we use the LIKE operator				
Select * from Students where Name like 'S%'				
Here, all the students records will be fetched whose name starts with S				
Wild Cards :				
While we do pattern matching, we use some wild cards to make SSMS understand what exactly we are looking for		
% - Specifies 0 or any number of characters				
_ - Specifies only character				
[] - Specifies any characters to be matched which has been given with in the brackets				
[^] - Specifies any other characters to be matched which has been given with in the brackets				
Ex - Select * from Students where Name like '[SAT]%'				
This will fetch all the student records whose name starts with either S or A or T				
If we want to fetch all the student records except the students whose name starts with S or A or T, then we just need to add ^ before passing the characters
Ex - Select * from Students where Name like '[^SAT]%'				
				
Top and Top Percent :				
This top and top percent is used to select some particular number of top rows from the resultant dataset			
Select top 1 * from Students - This will fetch the first row of the resultant Students table				
Select top 50 percent * from Students - This will fetch the 50 percent of records from the Student table			
				
Group by :				
Whenever we want to find some result for a particular group (in group level), then we use Group by clause			
When we give a group by clause, internally it creates group for the independent values of that field and thus we cannot print any indivisual field values
We can only give the fields in the select clause which are used either in aggregate function or is in the group by clause		
Select city, sum(salary) from Resources group by city				
Here, we are finding the total salaries of all associates based on different cities				
				
When we want to use any conditionality in field level, then we use where				
When we want to use any conditionality in group level, then we use having				
				
JOINS :				
Joins are used when we want to show values of the fields which are in different tables				
These are primarily of three types :				
1. INNER JOIN - Inner join is the join where only matching records comes from both the tables				
2. OUTER JOIN - Outer join is the join where matching as well as non-matching records comes from both the tables		
3. CROSS JOIN - A cross join gives a cartesian product of the two tables involved in the join. It should not have an ON CLAUSE	
				
Outer Join is again divided into three types :				
i> Left Join or Left Outer Join - Left join or Left Outer Join is the join where all the matching rows are returned along with the non-matching rows of the LEFT table
ii> Right Join or Right Outer Join - Right join or Right Outer Join is the join where all the matching rows are returned along with the non-matching rows of the RIGHT table
iii> Full Join or Full Outer Join - Full join or Full Outer Join is the join where all the matching rows as well as the non-matching rows are returned from both the tables
				
Advanced or Intelligent Joins :				
Whenever we want only the non-matching rows, then we use Advanced or Intelligent Joins				
This join is nothing but an outer join (can be left or right or full) with a condition				
				
Self Join :				
When we join a table with itself, then we call it a Self Join				
Now, Self Join can be :				
i> Inner Self Join				
ii> Outer Self Join (Left, Right and Full)				
iii> Cross Self Join				
				
Ways to replace NULL values :				
We can use either ISNULL function or COALESCE function or CASE statement to replace NULLS				
				
ISNULL function is used to replace NULL values with any other value				
It takes two parameters :				
i> Value to be checked (NULL or NOT)				
ii> Replacement Value (Value which will be used incase the first value is NULL)				
Select ISNULL(NULL, 'No Manager') as Manager				
Here, the Manager column will have the value as 'No Manager' since the passed value is NULL				
Select ISNULL('PRAGIM', 'No Manager') as Manager				
Here, the Manager column will have the value as 'PRAGIM' instead of 'No Manager' since the passed value is NOT NULL		
				
COALESCE function can also be used to replace NULL values but it has much more functionality than to replace NULLS		
It takes multiple parameters and returns the first NON-NULL value from the passed values				
				
CASE statement can also be used to replace NULLS but it is mostly used when there is a conditionality involved and based on the conditions we need to show some specific values
Syntax of a CASE statement :				
CASE when <expression condition> then <value when TRUE> else <value when FALSE> END				
				
Union and Union All :				
When we want to combine the results of two different select queries, then we use Union or Union All depending on the situation	
Union All takes all the records from the first table and then combines with all the records of the second table			
Union gives sorted distinct records after combining all the records from both the tables				
				
An Union query runs a little slower than Union All query since it has to perform Distinct Sort				
Distinct Sort will only be performed in a union query, when all the column values of the different queries are same
	
If you want to check the time taken by both the queries, you can see that by checking the Estimated query execution plan		
To check the Estimated query execution plan --> Ctrl + L				
				
NOTE : For UNION and UNION ALL to work, the number, data types and the order of columns in both the select statements should be same	
				
If you want to sort the results of a UNION or UNION ALL, use the ORDER BY clause in the last select statement		
				
Difference between UNION and JOIN :				
Union combines rows from two or more tables while Join combines columns from two or more tables			
				
Stored Procedures :				
A stored procedure is a group of T-SQL statements which runs together at a time. This is required when we need to perform a specific task repeatatively.
When we need to write and execute specific query or queries again and again we save those queries as a package together which is known as stored procedures or proc
Now, whenever we need to execute those queries, we can simply call the proc and those particular set of statements will execute together	
				
The keyword to create a procedure is :				
CREATE PROCEDURE <Procedure_Name>
AS
BEGIN
      <Body>
END				
				
There are two types of procedures in SQL Server :				
i> System Stored Procedures - These are pre-defined and are used for the proper functioning of MS SQL Server		
ii> User Defined Stored Procedures - These procedures are created by the developers for any specific task			
				
The name of the system SP starts with sp and usually it is recommended to start the name of the user defined SP with usp		
				
To execute a SP just write the name of the SP :				
<SP_Name>				
exec <SP_Name>				
exececute <SP_Name>				
				
Parameterized Stored Procedure is a procedure which takes parameter(s) before getting executed or pass parameter(s) after getting executed	
CREATE PROCEDURE <Procedure_Name>
@input  int,
@output  varchar(50) output
AS
BEGIN
      <Body>
END				
				
This is a parameterized SP which takes one parameter as input and passes one parameter as output			
				
To modify an existing stored procedure : We use ALTER
ALTER PROCEDURE <Procedure_Name>
@input  int,
@output  varchar(50) output
AS
BEGIN
      <Modifications/Changes - Body>
END
				
To delete an existing stored procedure : We use DROP				
DROP PROCEDURE <Procedure_Name>				
				
If we want to Encrypt the text of the Stored Procedure, then we just need to write "WITH ENCRYPTION" after declaring the variables of the parameterized SP
ALTER PROCEDURE <Procedure_Name>
@input  int,
@output  varchar(50) output
With Encryption
AS
BEGIN
      <Modifications/Changes - Body>
END				
				
The moment we encrypt an SP, if somebody tries to open the SP or view the text of the SP using sp_helptext system procedure, then it will not show the contents instead it will throw an error message saying "<SP_Name> is encrypted"
				
To call a SP which has both input and output parameters, we need to declare a variable in order to receive the output and then call it :
declare @out  int
execute <sp_name> <input_value>, @out out/output
print @out				
				
System stored procedures are default present and are used for specific purposes			
There are many system stored procedures for different tasks :				
i> sp_helptext <User_Proc_Name>  <-- Helps to get the contents of the User Defined SP				
ii> sp_help <DBO_Name>  <-- To view the information of the database object like parameter names, datatypes etc. This SP can be used with any database object like tables, triggers, stored procedures etc
iii> sp_depends <DBO_Name>  <-- To view the dependencies of the Stored Procedures				
iv> sp_renameDB <DBO Name>  <-- To change the name of the used defined databases
				
When we execute a stored procedure, we always get a return value which is of type integer. The return value of 0 indicates success and a non-zero value indicates failure
				
Stored Procedures with return values :				
Instead of creating an output parameter, we can make use of the return statements in Stored Procs				
CREATE PROCEDURE <Procedure_Name>
AS
BEGIN
      return (Select count(*) from Employees)
END				
				
This will return the count of total employees present in the Employees table				
				
In order to call the SP which make use of the return statements :				
declare @out  int
execute @out = <Procedure_Name>
print @out				
				
Here, we are containing the value returned by the Stored Procedure in the out variable and then we are printing it out		
				
There can be optional parameters in stored procedures as well				
To make the parameters optional in the stored procedure, we simply initiate the parameter with any default value so that when we donot supply any value to the procedure, it takes the default values
CREATE PROCEDURE <Procedure_Name>
@input  int = NULL
AS
BEGIN
      <Modifications/Changes - Body>
END				
Here, if we donot pass any value to the input parameter, then by default the procedure takes the input value as NULL				
				
NOTE :				
We cannot return any non-integer type values using the return statement. If we do we will always get a conversion failed error message	
We cannot return more than one integer value using the return statement
				
Advantages of Stored Procedures :				
1> Execution plan retention and reusability				
Whenever we run any sql query, four things happen :				
i> It checks the syntax of the query				
ii> It compiles the query				
iii> It generates an execution plan				
iv> It executes the query				
				
So once the plan is generated, SSMS cached the plan and therefore when the same query is executed again, it follows the same execution plan that has been already generated
				
An Adhoc or inline query can also retain the execution plan but even a small change in the query like change in the value of the condition, the execution plan gets changed
But in case of a SP, since there is no change in the body i.e. the queries written under the SP, it retains the execution plan and reusability of it	
				
2> Reduces traffic network				
Since it resides as an object in the Server itself, the line of code that has to pass over the network is only the name of the procedure and parameters it takes
Whereas if we want to write the whole logic of any functionality, then we need to pass a lot of sql statements to the server over the network	
Hence, we can say that using Stored Procedures helps in reducing network traffic as well thereby increasing the performance	
				
3> Code reusability and better maintainability				
If say a specific functionality is used by multiple applications, then if there is some change required in any point of time, we need to make the changes in each and every application which uses it
So, it is better to have the common functionality in one place (can be stored as a Stored Proc) so that during change, the only place where the change will be needed is in the SP
Thus we can say that for better code reusability, we can make use of stored procs and it is very easy to maintain as well		
				
4> Better security				
Provides better security by avoiding SQL Injection Attack				
				
SQL Injection Attack : (Most common in web applications)				
When we use Sql Statement by concatenating string in the where clause, there is a chance of SQL Injection attack		
let's say we write a query :				
Select * from tblProduct where name = '<Type product name>'				
Any product name you type in between the quotes you will get the details of that product				
Now, if someone writes the below line in between the quotes :				
Pen'; delete from tblProduct --				
Essentially, the final query becomes :				
Select * from tblProduct where name = 'Pen'; delete from tblProduct --'				
Select * from tblProduct where name = 'Pen'; delete from tblProduct --'				
Here, the highlighted query will be treated as one single independent query and then the second query will run :		
Select * from tblProduct where name = 'Pen'; delete from tblProduct --'				
Here, the highlighted query will be treated as one single independent query again thus deleting all the records from the table	
This is called SQL Injection Attack. Since in Stored Procedures, we always pass the values using parameters, the value which is passed will be considered as the value thereby reducing the risk of SQL Injection Attack
Select * from tblProduct where name = @name				
				
String Functions :				
Commonly used string functions :				
i> ASCII(<Value>)				
Returns the ASCII code of the character (Value lies between 0 and 255)				
If you pass a string instead of a character, then it will return the ASCII value of the first letter of the string			
ii> CHAR(<value>)				
Returns the character associated with the value passed which is considered as an ASCII value				
iii> LTRIM(<value>)				
Removes spaces from left side of the field values				
iv> RTRIM(<value>)				
Removes spaces from right side of the field values				
v> UPPER(<value>)				
Converts the value passed into upper case				
vi> LOWER(<value>)				
Converts the value passed into lower case				
vii> REVERSE(<value>)				
Reverses the value passes				
For ex - SAM will become MAS				
viii> LEN(<value>)				
Returns the length of the passed string				
ix> LEFT(<value>,<number_of_characters_to_be_extracted>)				
Extracts characters from the left side				
x> RIGHT(<value>,<number_of_characters_to_be_extracted>)				
Extracts characters from the right side				
xi> CHARINDEX(<character_to_search>,<string_where_you_want_to_search>,<starting_position>)			
This returns the position of the character that has to be searched in the passed string				
xii> SUBSTRING(<expression>,<start_position>,<length>)				
This extracts part of a string from another string				
xiii> REPLICATE(<String_to_be_replicated>,<Number_of_times_to_be_replicated>)				
This repeats the passed string passed number of times				
xiv> SPACE(<Number_of_space>)				
This creates a space of width depending on the passed value				
xv> PATINDEX(<Pattern>,<Expression>)				
This returns the starting position of a given pattern --> PATINDEX("%@gmail.com",Email)				
xvi> REPLACE(<Expression>,<String_to_be_replaced>,<Replacement_Value>)				
Replaces all occurances of a specified string with a given string				
xvii> STUFF(<Expression>,<Starting_position>,<Number_of_characters_to_be_replaced>,<Replacement_Value>)		
Basically this function is used of masking purpose				
				
Difference between CHARINDEX and PATINDEX :				
Wild cards can be used with PATINDEX but we cannot use wild cards with CHARINDEX				
				
Cast and Convert functions :				
Both these functions are used to convert the type of the variable				
Syntax of Cast :				
cast(<expression> as <datatype>)				
Syntax of Convert :				
convert(<datatype>,<expression>,[<style>])   <-- style is optional				
				
NOTE : Cast is universal (ANSI standard) and thus can be used in other databases as well but Convert is specific to SQL Server 	
				
Functions :				
There are two types of functions - 				
System functions				
User Defined functions (UDF)				
				
There are three types of User Defined Functions :				
i> Scalar Functions				
ii> Inline table-valued functions				
iii> Multi-statement table-valued functions				
				
i> Scalar functions :				
In-built system functions are scalar functions				
A function should always return a value. It doesnot matter whether the function takes any inpur parameters or not		
The function which returns a single (scalar) value is called a scalar function. It may or maynot have parameters.		
Function cannot return the given datatypes : text, ntext, image, cursor and timestamp				
				
To create a function, we use :   (A function can take a maximum of 1024 parameters)				
create function <function_name>[(@parameter1 datatype1, @parameter2 datatype2……@parameter1024 datatype1024)]
returns <return_datatype>
as
begin
           <body of the function>
end				
create function <function_name>(@DOB date)
returns int
as
begin
           declare age int
           set age = Date(getdate()) - Date(DOB)
           return age
end				
To call a scalar function, use select and then the name of the function similar to how we call the built-in functions but we need to specify the full name - "dbo.<function_name>"
				
We can use the scalar functions in the select as well as where clause of a query				
Procedures can also be used to achieve the same output but we cannot use procedures in select and where clause of a query	
				
To alter :				
alter function <function_name>				
				
To delete :				
drop function <function_name>				
				
ii> In-line table-valued function :				
This returns a table				
Syntax to create :				
create function <function_name>[(@parameter1 datatype1)]
returns table
as
return (select query)				
create function fn_GetTableByGender(@gender varchar(10))
returns table
as
return (select * from tblMember where gender = @gender)				
				
Note : Here, we donot add begin and end after function declaration				
				
Since in-line table-valued function returns a table, it is treated as table and is called in the FROM clause of a query		
select * from fn_GetTableByGender("Male")				
				
This is similar to parameterized views since only selected columns will only be fetched after using in-line table-valued function	
				
iii> Multi-statement table-valued function :				
This is similar to in-line table-valued function but here in mutli-state table-valued function, the structure of the table is also defined which is to be returned (In-line table-valued function doesnot have any defined structure to return)
Syntax to create :				
create function <function_name>[(@parameter1 datatype1)]
returns @table table (id int, name varchar(10), DOB(date))
as
begin
          insert into @table
          select ID, Name, cast(DateOfBirth as date) from tblEmployee

          return
end				
				
Select * from <function_name>				
				
It is mandatory to pass a parameter to Table-Valued functions whereas in scalar functions, parameter passing is optional		
We can issue update and delete query to in-line table-valued function but the same cannot be done in multi-statement table-valued function	
				
Deterministic Function :				
A deterministic function is a function whose result can be determined and is always same untill and unless the input or the state of the database is changed
All aggregate functions are deterministic functions				
ex - SQUARE(), POWER(), SUM(), AVG(), COUNT()				
				
Non-Deterministic Function :				
It is opposite to deterministic function. It may return different values even if the input and the state of the database is not changed	
ex - GETDATE(), CURRENT_TIMESTAMP				
				
RAND() function can act as deterministic as well as non-deterministic function depending on the fact that whether the seed value has been supplied to the function or not
If seed is supplied then it will always return same value otherwise it will return different values				
				
The way we used to encrypt stored procedures, we can encrypt functions in the same way as well :				
WITH ENCRYPTION				
This will prevent to open the function graphically as well as we cannot see the text of the function using system SP - sp_helptext	
				
Schema Binding :				
Whenever we create a function which has an underlying dependent table in it, then it is good practice to use the schemabinding option during its creation
When we don't specify schema binding option, then if you try to delete the underlying table or make any change in it, you will be allowed to do it
When you specify the schema binding option, then you cannot delete the underlying table connected to the function		
Syntax :				
create function <function_name>(@Clntcode char(05), @Certno char(10))
returns varchar(50)
with schemabinding
as
begin
          select name from tblEmployee where clntcode = @Clntcode and certno = @Certno
          return name
end				
				
Temporary tables :				
A temporary table is a table which doesnot gets created permenently in the database but gets created temporarily in the tempDB system database of SQL Server
Every temporary table should start with '#'				
select * from <database_name>..sysobjects where name like 'abc%'				
The scope of a temporary table is within a single connection in which it has been created				
There are two types of temporary tables :				
i> Local Temporary table - starts with single '#'				
ii> Global Temporary table - starts with double '##'				
				
The local temporary table is immediately dropped as soon as the connection that created it is closed			
If we create a temporary table inside a stored procedure, then that temporary table gets dropped as soon as the procedure execution gets completed
Global temporary tables are visible and can be accessed from all the connections and are dropped only after the last connection referring the table is closed
				
Indexes :				
It is very similar to the index which is present in the books				
Basically it is used to improve the performance of a query on a specific table where index is defined				
An index stores the address of the row which has to be reached when a specific condition is provided rather than looking in the entire table	
Syntax to create :				
Create index <index_name>
on <Table_Name> (<Column_Name> ASC/DESC)				
This will create an index on the given table based on the column values arranged in either ascending or descending order and will have a row address pointing to the row of the main table
				
sp_helpindex helps to get all the indexes created on the given table				
				
Dropping an index :				
Drop index <table_name_on_which_index_is_created>.<index_name>				
				
A primay key constraints automatically creates a clustered unique index based on the primary key of that table if there is no clustered index already present for that table
We can have only one clustered index per table however there can be many fields which is indexed for a particular table		
Composite Index - A composite index is an index which has multiple columns in it				
Based on the clustered index defined, the result is shown when a query is executed on the table				
				
Clustered index :				
A clustered index is an index which helps to arrange the data in the table in any specific order based on any specific field		
That's why there can be only one clustered index for a table				
create clustered index <index_name>
on <table_name>(<column_name(s)> ASC/DESC)				
				
Non-clustered index :				
Here the data is stored in one place and the index is stored in another place				
The index will have pointers pointing to the address of the records in the original table				
				
NOTE :				
Non-clustered index does not have any effect in the order of storage of the data in the original table but a clustered index have.	
Clustered index are faster than non-clustered index since it doesnot have to lookback				
Non-clustered index requires more disk space since the index table gets created separately from the original table		
				
Behind the scenes a primary key constraint uses the unique clustered index to enforce the uniqueness of the key column		
If we delete the primary key clustered index which gets created automatically, then the primary key constraint also gets deleted resulting in duplicate entry in the table
The same is the case with unique constraints as well				
				
Differences between Unique constraint and Unique index :				
There are no major differences between the two. When you create a unique constraint on a column, behind the scenes it creates an unique index for that column
				
An unique constraint or unique index cannot be created in an existing table having duplicate values 				
DML statements can get affected when you have too many indexes since it has to do the same for all the indexes along with the original table	
				
Covering query :				
If you have selected the columns which is there in the index of the table, then it is not required to lookback in the main table.	
It can be accessed from the index itself and those queries which has all the columns in the select statement which are specified in the index is known as a covering query
				
A clustered index always covers a query since it contains all of the data in a table				
				
Views :				
A view is nothing more than a saved query. It is also considered as a virtual table.				
Syntax :				
create view <view_name>
as [Any select statement]				
				
Whenever you run a view :				
Select * from <view_name>				
This will run the select query which is written under the view declaration				
				
Uses :				
Helps to provide limited access to rows and column thereby providing security to the data stored				
Can be used to present aggregated data by hiding the detailed data				
				
Whenever you issue an update, insert or delete command in a view, it acctually affects the underlying base table and does the operations in that table only
				
You can have a view with more than one underlying table in it				
In such cases if you update, the data may get updated wrongly and in order to correct that "INSTEAD OF" triggers are used	
NOTE : The data will get updated only if any one of the base tables is affected otherwise if multiple base table is going to get affected with any of the DML query (Insert, Update, Delete), then SQL Server will not allow the update to happen
ALWAYS REMEMBER, VIEWS CAN BE UPDATED ONLY IF IT HAS ONE BASE TABLE OR AFFECTING ONE BASE TABLE OUT OF MANY BASE TABLES
				
Indexed Views (Materialized views in Oracle) :				
Views are basically a stored SQL statement which runs every time when you use the view				
These views donot contain any data in itself rather behind the scenes, it gets the data by running the saved query		
Now, if say the query which runs behind the scenes whenever the view is called is huge and contains large data, then it will significantly reduce the performance of the query since it has to run the saved query again and again whenever the view is used
In order to resolve that, we have something called indexed views				
Indexed views are the views which contains data in itself. Now if you use the indexed view, it fetches data from itself and not from the base tables using the saved query.
Since views doesnot contain any data in itself, the first index that has to be created in the view should be unique clustered index.	
Thereafter if any changes done in the base tables, those changes will be automatically reflected and saved in the indexed view as well and thus on calling the view, we will get the updated record values from the view itself
				
Syntax to create clustered indexed view :				
Create unique clustered index uix_<view_name>
on <view_name> (<column_name>)				
				
Rules to remember before creating indexed views :				
i> Make use of COUNT_BIG function instead of count function				
ii> Create the view by using the WITH SCHEMABINDING option.				
iii> Create the unique clustered index on the view.				
iv> The view must reference only base tables that are in the same database as the view.				
				
Limitations :				
i> We cannot create parameterized views				
ii> Constraints cannot be associated with a view				
iii> You cannot use an orderby clause in the view defination without TOP or FOR XML				
iv> Views cannot be based on temporary tables				
				
Triggers :				
There are three types of triggers in SQL Server :				
i> DML Triggers				
ii> DDL Triggers				
iii> Logon Triggers				
				
Triggers are the database objects or stored scripts which gets auto fired in response to certain events			
Events can be DML or DDL or Logon events				
				
DML Triggers can be again classified into two types :				
i> After Triggers (FOR Triggers)				
ii> Instead Of Triggers				
				
After Triggers are the triggers which gets fired after the triggering action (Insert, Update or Delete)				
Instead Of Triggers are the triggers which gets fired instead of the triggering action (Insert, Update or Delete)			
				
Syntax to create a trigger :				
create trigger <trigger_name>
on <table or view_name>
FOR <Type/Action - INSERT/UPDATE/DELETE>
AS
begin
         <body_of_the_trigger>
end				
create trigger tr_Member_ForInsert
on Member
FOR INSERT
AS
begin
         insert into MemberHist Select * from inserted
end				
				
This is an insert trigger which gets triggered in response to an insert action in the Member table				
				
Magic tables : Triggers have magic tables (inserted/deleted) which are available in context of a trigger only			
These are the tables which holds all the field values temporarily and gets destroyed after the execution of the trigger		
Now its important to understand that :				
when we raise an INSERT event, SQL Server generates an inserted table which contains all the field values of the data that is going to get inserted
when we raise a DELETE event, SQL Server generates a deleted table which contains all the field values of the data that is going to get deleted
when we raise an UPDATE event, SQL Server generates both inserted and deleted tables :				
Inserted table contains all the field values that are going to get updated in the table (NEW FIELD VALUES)			
Deleted table contains all the field values that are going to get replaced with the new value (OLD FIELD VALUES)		
				
Instead of triggers are mainly used for views, in order to make the insertions, updations and deletions properly in the base tables	
So, instead of the acctual insert query, the instead of trigger will run and update the base tables accordingly using the magic tables provided by SQL Server
				
Syntax to create :				
create trigger tr_<name>
on <table/view_name>
instead of <type/event - INSERT/UPDATE/DELETE>
AS
begin
          <body of the trigger>
end				
create trigger tr_Employee_InsteadOfInsert
on Employee
instead of INSERT
AS
begin
          Select * from inserted
end				
				
This is an instead of insert trigger which gets triggered in of the insert action in the Member table. This will show the values of the insert query but will not be inserted in the Employee table
				
Table variable :				
A table variable is a variable whose type is of table				
It is a variable which has the capability to store a table in it				
The way to declare a table variable is similar to how we declare any other variable				
				
Syntax to create a table variable :				
declare @table1 table (<column_name1> <data_type>, <column_name2> <data_type>…<column_namen> <data_type>)		
				
Syntax to use a table variable :				
insert @table1 Select <column1>, <column2>…<columnn> from Employee				
select * from @table1				
				
Just like temporary tables, a table variable is also created in the tempDB				
The advantages of table variable is that it can used to pass a table between procedures				
				
Derived tables :				
A derived table is a table which is not an acctual table but we consider the result of a query as a table.			
Select max(Salary) from (Select salary from Employee) as t				
				
CTE (Common table expression) :				
It is almost similar to Derived tables only				
Here also we are considering the result of a query as a table but here, we start with the WITH keyword			
Syntax to create :				
WITH <CTE_Name>[(<ColumnName1>, <ColumnName2>…., <ColumnNamen>)]    <-- This is optional
as
(
     <body of the CTE>
)				
WITH EmployeeCount[(DeptName, DeptID, TotalEmployees)]
as
(
     Select DeptName, DeptID, count(*) from Employees group by DeptName, DeptID
)				
				
A CTE can be referenced within a select, insert, update or delete statement that immediately follows the CTE			
We can create multiple CTE's with a comma in between them				
				
Updatable CTE : In certain scenarios, we can update the CTE but in certain scenarios, we cannot update the CTE		
When we update a CTE, the underlying base table gets updated				
If a CTE is based on one base table, then it is possible to update the CTE				
If a CTE is based on multiple tables, and the update is affecting only one base table, then the CTE is updatable otherwise it is not	
				
Normalization :				
Database normalization is a step by step process of organizing data to minimize data redundancy which in turn leads to data inconsistency	
				
Problems of data redundancy :				
i> More disk storage				
ii> Data inconsistency				
iii> DML queries can become slow				
				
Database normalization is a step by step process				
There are 6 Normalized Froms - 1NF thru 6NF				
In order to make a table in any normalized form, there are certain rules to follow :				
				
NOTE : Most of the databases are in 3rd normal form (3NF)				
				
1NF :				
i> The data should be atomic				
ii> The table should not contain repeating column groups				
iii> There should be a primary key in a table to uniquely identify each record
				
2NF :				
i> The table meets all the conditions of 1NF				
ii> Move redundant data to a separate table				
iii> Create relationships between tables using foreign key				
				
3NF :				
i> The table meets all the conditions of 1NF and 2NF				
ii> Doesnot contain columns that are not fully dependent upon the primary key				
				
Error Handling :				
There is a system function - @@ERROR which helps to identify whether there is any error occurred in the last statement executed or not	
				
If there is any error occurred in the last statement executed, its value get set to a non-zero value. The value 0 indicates that the statement executed above is successful
				
These system functions which starts with @@ are known as golbal variables but their behaviour is not like any variable instead they act very similar to function
To throw an error we use a function called : RAISERROR				
This helps to throw a custom error				
This takes 3 parameters :				
i> Error message				
ii> Severity level				
iii> State				
Syntax :				
RAISERROR('Error occurred',16,1)				
Severity level in most cases is 16 which indicates that the error is a general error which can be corrected by the user		
State state can only be a value between 1 and 127 but generally we use 1				
				
@@ERROR always gets reset to zero immediate after the executing the statement. If you want to get the error, you have to check it in the immediate next statement
				
Try/Catch block for error handling :				
To handle error include the bunch of code inside a Begin Try and End Try block				
Begin Try
     <Block of SQL Code>
End Try				
If any error happens inside the Try block, the control immediate jumps to Begin Catch and End Catch block			
Begin Catch
     <Block of SQL Code>
End Catch				
Syntax :				
Begin Transaction
Begin Try
     <Block of SQL Code>
     commit transaction
End Try
Begin Catch
     rollback transaction
     RAISERROR('Error Occurred',16,1)
End Catch				
				
There are some system functions which helps us to identify a number of parameters :				
i> ERROR_NUMBER() - Returns the number of the error				
ii> ERROR_MESSAGE() - Returns the message of the error				
iii> ERROR_PROCEDURE() - Returns the procedure name where the error occurred				
iv> ERROR_STATE() - Returns the state of the error				
v> ERROR_SEVERITY() - Returns the severity of the error				
vi> ERROR_LINE() - Returns the line number where the error has actually happened				
				
To find the line number where the error has occurred, press CTRL + G and then give the line number				
				
NOTE : These functions are only available in context of the CATCH block only. If you want to use the functions outside of a CATCH block you will end up getting the value NULL for each functions				
				
If there are no exceptions, the control goes to the next statement of the catch block				
				
TRY/CATCH block cannot be used in User Defined Functions				
				
Transactions :				
A transaction is a group of commands that change the data stored in the database. A transaction is treated as a single unit.		
A transaction ensures that either all the commands are successful or none of them are successful i.e. if one command fails, then all commands should be treated as failed
				
This is required to maintain the integrity of the database				
				
Syntax :				
Begin Transaction
     process database command
     check for errors
     if errors occurred
           rollback transaction
     else
           commit transaction				
				
When we make any transaction, and we donot end the transaction meaning we neither commit the transaction nor we rollback the transaction, then the changed value can be seen by the person connected to the same connection only
If another person of a different connection want to access the same table where the transaction has run, it will try to fetch the records but will not be able to do so untill the transaction is finished
So, if you want to see any uncommitted data, you need to set the isolation level to READ UNCOMMITTED :			
Set transaction isolation level read uncommitted				
				
NOTE : By Default the isolation level of SQL Server is Read Committed				
				
A successful transaction must pass the ACID test				
It should be :				
i> Atomic - All statements in the transaction is either completed successfully or they were all rolled back. The task is never left half-done	
ii> Consistent - All data touched by the transaction is left in a logically consistent state				
iii> Isolated - All the transactions should work as a single unit meaning transaction A should not interfere transaction B and vice versa (We can get isolation level using locking methods)
iv> Durable - Once a change is made, it is permanent. If a system error or power failure occurs before a set of commands is complete, those completed commands are undone and the data is restored to its original state once the system begins running
				
Sub-queries :				
The query which is present inside a paranthesis is called a sub-query.				
The sub-query run first and then the main or the outer query is run. This sub-query is also known as inner query		
Now, this sub-queries can be very easily replaced with joins				
				
Sub-queries can be nested upto 32 levels				
				
Correlated Sub-queries : If the sub-query depends on the outer query for its values, then it is known as correlated sub queries	
A correlated sub-query gets executed for every row of the outer query				
				
Select Name, (select sum(QtySold) from tblProductSold where PID = tblProduct.ID) as TotalQtySold from tblProduct		
Here, the sub-query is a correlated sub-query since it depends on the outer query for its values. It cannot be run independently as well	
Here, the sub-query is using the ID field of the tblProduct table which is not used in the inner query but is present in the outer query	
				
Creating a large table with random data for performance testing :				
information_schema.tables  <--  contains list of all the tables along with the database in which it exists			
select * from information_schema.tables				
				
Exists returns boolean value (True or False)				
if (Exists(Select * from information_schema.tables where table_name = 'tblProduct'))
       drop table tblProduct
create table tblProduct(<table defination>)				
				
Here, this query checks if the table exists or not. If it exists, then this exists function will return TRUE and the control will go inside the if statement and finally it drops the table
				
To generate random numbers use the function - RAND()				
				
To clear the query cache and the execution plan cache we need to use DBCC commands :				
CHECKPOINT;				
GO				
DBCC DROPCLEANBUFFERS;     <-- clears query cache				
GO				
DBCC FREEPROCCACHE;     <-- clears execution plan cache				
GO				
				
In general we can say that JOINS work faster than SUB-QUERIES				
				
Cursors :				
When you want to make any operation on all the rows or a specific set of rows one by one, then we use cursors		
				
NOTE : Cursors are very bad for performance and can be avoided very easily by using joins				
				
There are different types of cursors in SQL Server :				
i> Forward-Only				
ii> Static				
iii> Keyset				
iv> Dynamic				
				
Syntax : (declaring a cursor)				
Declare <name_of_the_cursor> cursor for
select ID, NAME from tmember				
				
opening cursor : 				
open <cursor_name>				
The moment you open a cursor, the select query runs and the result set of the query is kept in hold				
				
fetching values from cursor :				
fetch next from <cursor_name> into @ID, @NAME				
This will fetch the first value into the declared variable ID and Name				
				
@@FETCH_STATUS - This helps to get the fetch status (Always returns 0 (zero) whenever there are values returned		
				
closing cursor :				
close <cursor_name>				
As soon as you close the cursor, it is going to release the result set				
				
deallocating cursor :				
deallocate <cursor_name>				
				
Listing all the tables in SQL Server :  (65)				
There are 3 system views with which we can achieve the same :				
i> SYSOBJECTS				
ii> SYS.TABLES				
iii> INFORMATION_SCHEMA.TABLES				
				
using SYSOBJECTS :				
select * from sysobjects where xtype = 'U'				
Here, sysobjects is the view which has all the objects of the database and xtype field is used to identify the type of the object	
if the xtype is 'U', that means the objects are User Defined Tables				
				
using SYS.TABLES :				
select * from sys.tables				
This will only fetch the tables that are there in the database				
if we use sys.views, that will only fetch all the available views				
if we use sys.procedures, that will only fetch all the available procedures				
				
using INFORMATION_SCHEMA.TABLES :				
select * from information_schema.tables				
This will fetch all the tables and views present in the database				
for the tables the TABLE_TYPE field will have the value BASE TABLE				
for the views the TABLE_TYPE field will have the value VIEW				
				
DateTime Functions :				
datetimeoffset datatype - YYYY-MM-DD hh:mm:ss.[nnnnnnnn] [+/-] hh:mm				
This is offset (UTC Time Zone - Coordinated Universal Time) - The one which is bolded above				
It is the central time with which the world regulates clocks and time				
Day Light Savings - Sometimes during summer the time in some parts of the world moves ahead of the UTC but in winters they come back to the UTC time
Thus though there are some differences in time in some parts, the offset is not used for those areas since Day Light Savings are not considered while calculating time wrt t o UTC
				
GETDATE() - To get the system date and time (commonly used)				
CURRENT_TIMESTAMP - ANSI SQL equivalent to GETDATE()				
SYSDATETIME() - More fractional seconds precision				
SYSDATETIMEOFFSET() - More fractional seconds precision + Time zone offset				
GETUTCDATE() - UTC Date and Time				
SYSUTCDATETIME() - UTC Date and Time with more fractional seconds precision				
				
ISDATE() - To find valid date and time (returns 1 if valid otherwise 0) [For datetime2 values ISNULL returns 0]			
DAY() - Returns the day number from the given date				
MONTH() - Returns the month number from the given date				
YEAR() - Returns the year number from the given date				
DATENAME(datepart, date) - Returns the weekday name or month name etc depending on what you pass in the datepart argument	
DATEPART(datepart, date) - Returns the weekday number etc depending on what you pass in the datepart argument		
				
The only difference between DATENAME and DATEPART is DATENAME returns nvarchar while DATEPART returns an integer	
				
DATEADD(datepart, numbertoadd, date) - Returns the datetime, after adding specified numbertoadd, to the datepart specified of the given date
DATEDIFF(datepart, startdate, enddate) - Returns the count of the specified datepart boundaries between the given two dates	
				
Mathematical Functions :				
				
ABS(expression) - Returns the absolute value of the entered number (positive value)				
CEILING(expression) - Returns the next lowest whole number than the passed value				
FLOOR(expression) - Returns the previous highest whole number than the passed value				
POWER(expression, power) - Returns the power of a number				
SQUARE(expression) - Returns the squared value of the entered number				
SQRT(expression) - Returns the square root value of the entered number				
RAND([expression]) - Returns a random number between 0 & 1				
NOTE : Here the expression is optional but if you supply a seed value as the expression, then it generates the same number again and again	
ROUND(expression, length [, function]) - Returns rounded or truncated value of the passed number				
NOTE : Here the function is optional. By default it is 0 which means rounding otherwise if you pass a non-zero number to it, then it truncates the remaining values and convert them to 0
				
When you pass negative value in length parameter, then it rounds the passed number from the left side of the decimal point	
				
OBJECT_ID('object name') - Returns the ID of the passed object				
				
NOTE : You cannot change the datatype of any column where records are already present using the designer. In order to do so, you can use the alter sql query to change the datatype
				
Merge Statements :				
Merge statements allow us to perform insert, update and delete operations in one statement only.				
To use a merge statement :				
1. We need two tables : 				
         a> Source Table - Contains the changes that needs to be applied to the target table				
         b> Destination Table - The table that requires changes (Inserts, Updates and Deletes)				
2. Both the tables should have a common column and the way in which the rows matchup, the insert, update or delete operations are performed
				
Syntax :				
MERGE INTO [TARGET] as T
USING [SOURCE] as S
on [JOIN CONDITIONS]
WHEN MATCHED THEN
Action (Insert/Update/Delete)
WHEN NOT MATCHED BY TARGET THEN
Action (Insert/Update/Delete)
WHEN NOT MATCHED BY SOURCE THEN
Action (Insert/Update/Delete);				
				
Here, we have two tables : Source Table and Destination Table : ------------------------------------------------------------------------>	Source Table	Destination Table
We will perform update when there is a match in the ID column											ID	Name	ID	Name
We will perform insert when the ID in the source table is not present in the destination table							1	Mike	1	Mike M
We will perform delete when the ID in the destination table is not present in the source table							2	Sara	3	John
				
MERGE INTO [Destination Table] as T
USING [Source Table] as S
on [T.ID = S.ID]
WHEN MATCHED THEN
      Update set T.Name = S.Name
WHEN NOT MATCHED BY TARGET THEN
     Insert (ID, Name) values (S.ID, S.Name)
WHEN NOT MATCHED BY SOURCE THEN
     Delete;				
				
Using this merge query statement, we can make the Destination Table in sync with the Source Table			
				
NOTE : A Merge Statement must end with a semicolon otherwise the query will throw an error stating the same		
Also we can have only one NOT MATCHED condition (we can omit the NOT MATCHED BY SOURCE condition or the other one if not required)
				
SQL Server Concurrent Transactions : (70)				
Concurrent Transactions means multiple transactions running at the same time in the same database			
Allowing concurrent transactions is essential for performance but they may introduce concurrency issues when two or more transactions are working with the same data at the same time
				
Some of the common concurrency problems :				
i> Dirty Reads				
ii> Lost Updates				
iii> Nonrepeatable Reads				
iv> Phantom Reads				
				
One way to solve all the concurrency issues is to allow only one user to run a transaction at any point in time			
This isolation level is known as serializable				
With this kind of approach, all the users who has to run several transactions need to wait for a much longer time as all the transactions gets queued and only one transaction is allowed to run at a given time
This results in poor performance and the whole purpose of having a powerful database system is defeated			
Another way is to allow all the transactions to run concurrently at the same time				
This isolation level is known as Read Uncommited				
With this kind of approach, it may cause all sorts of concurrency issues				
				
To balance this, SQL Server provides different types of isolation levels so that you can balance the performance and handle the concurrancy issues depending on the application needs
The different Transaction Isolation Levels provided by SQL Server are :				
i> Read Uncommitted				
ii> Read Committed				
iii> Repeatable Read				
iv> Snapshot				
v> Serializable				
				
Isolation Levels :Concurrency Issues ----------------->	Dirty Reads	Lost Updates	NonRepeatable Reads	Phantom Reads
Read Uncommitted					Yes		Yes		Yes			Yes
Read Committed						No		Yes		Yes			Yes
Repeatable Read						No		No		No			Yes
Snapshot						No		No		No			No
Serializable						No		No		No			No

*************************************************************************************************************************************

* SQL Agent :

SQL Agent is a feature provided in SSMS to schedule jobs.
By default it is disabled, but in order to start using it, you have to start it.
Once the feature is started, you can see all the scheduled jobs under the jobs folder.
You can also monitor the jobs using the Job Activity Monitor feature.
You can create a new job by right clicking on Jobs folder and then rest of the steps are self-explanatory.


* BCP (Bulk Copy Program) :

This is the utility which can be used to bulk import and export data from SQL Server table to external files and vice versa.
The BCP Commands can be used in Command Prompt.
BCP Command :
BCP {DBTable | "Query"} {IN | OUT | queryout | Format} switches

IN is to import data from file to data
OUT is to export data from table to file
queryout is to export query result to file

You can also use the BCP commands from SQL Server in TSQL statements by using the system procedure xp_cmdshell
xp_cmdshell helps to execute BCP commands from SQL Server itself
Normally this system procedure - xp_cmdshell is disabled which can be checked by using another system procedure - sp_configure
First you need to enable the Advance setting using the proc sp_configure and then you can enable xp_cmdshell
To enable :
sp_configure 'Advance options',1;
reconfigure
sp_configure 'xp_cmdshell',1;
reconfigure

Now you can use xp_cmdshell to perform BCP operations :
declare @error int
exec @error = master..xp_cmdshell '[BCP Command]'
select @error

Now, the problem with BCP Command to export data is the next time when it executes, all the existing data of the file gets replaced with the new data everytime.
To solve this, we can use another temporary file and append the data to the original file :
exec master..xp_cmdshell 'type "PathWithTempFileName" >> "PathWithOriginalFileName"'

If you want to delete any file using cmdshell :
exec master..xp_cmdshell 'del PathWithFileName'

If the operation is successful, then @error will return 0, otherwise 1

For import, we have Bulk Insert function as well which can be used as an alternative :
Bulk Insert <DestinationTable> from <Path> with
(
	DATAFILETYPE = 'char',
	FIELDTERMINATOR = '\t',
	ROWTERMINATOR = '\n'
)


* Checking CPU Usage and fixing it : [INCOMPLETE]

If a query is running for long time and we don't know which query is taking the most of the CPU usage, then we can find the query using system stored
procedure - sp_who2 and kill it using the DBCC command.
Exec sp_who2
After executing this procedure, we need to look for SPID which is greater than 50.


* SQL Server Statistics : [https://www.youtube.com/watch?v=nqAOF1DFpyU] [INCOMPLETE]

Statistics in SQL Server is nothing but the information about the data.
When you run a query, behind the scenes an execution plan gets generated from the optimizer and this plan depends heavily on the statistics of the query.
If somehow the optimizer of the sql server knows the estimated number of rows to deal with, then it can come up with some plan which can be used to fetch data
as fast as possible and so this statistics come into picture which helps to get an idea of the estimated number of rows that has to be dealt with when executed.


* SQL Server Query Optimization & Performance tuning :
Performance tuning includes query optimization, SQL client code optimization, database index management, and in another sense, better coordination between developers and DBAs
If your tables are constantly hammered by INSERT, UPDATE, and DELETE, you should be careful when indexing—you could end up decreasing performance as all indexes need to be modified after these operations
Further, DBAs often drop their SQL indexes before performing batch inserts of million-plus rows to speed up the insertion process. After the batch is inserted, they then recreate the indexes
Execution Plan tool in SQL Server can be useful for creating indexes
Its main function is to graphically display the data retrieval methods chosen by the SQL Server query optimizer
To retrieve the execution plan (in SQL Server Management Studio), just click “Include Actual Execution Plan” (CTRL + M) before running your query
Afterwards, a third tab named “Execution Plan” will appear. You might see a detected missing index. To create it, just right click in the execution plan and choose the “Missing Index Details…”

Tips & Tricks :
i> Use of EXISTS() instead of Count()
ii> Avoid Coding Loops
iii> Avoid Correlated SQL Subqueries - Use Joins instead
iv> Select Sparingly
v> Wise Use of Temporary Tables (#Temp)
vi> Deleting temp tables to clear tempdb resources

EXISTS() stops processing as soon as it finds a matching row, whereas COUNT() has to count every row, regardless of whether you actually need that detail in the end

In SQL Server data that you see in the tables are in rows 



* Perfmon & Performance Monitoring (Performance Counters) :



* Transactional Replication :